{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Basics\n",
    "\n",
    "Welcome to the section on deep learning! We'll be using Keras with a TensorFlow backend to perform our deep learning operations.\n",
    "\n",
    "This means we should get familiar with some Keras fundamentals and basics!\n",
    "\n",
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We will use the Bank Authentication Data Set to start off with. This data set consists of various image features derived from images that had 400 x 400 pixels. You should note **the data itself that we will be using ARE NOT ACTUAL IMAGES**, they are **features** of images. In the next lecture we will cover grabbing and working with image data with Keras. This notebook focuses on learning the basics of building a neural network with Keras.\n",
    "\n",
    "_____\n",
    "More info on the data set:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "\n",
    "Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. variance of Wavelet Transformed image (continuous) \n",
    "2. skewness of Wavelet Transformed image (continuous) \n",
    "3. curtosis of Wavelet Transformed image (continuous) \n",
    "4. entropy of image (continuous) \n",
    "5. class (integer) \n",
    "\n",
    "## Reading in the Data Set\n",
    "\n",
    "We've already downloaded the dataset, its in the DATA folder. So let's open it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "data = genfromtxt('../DATA/bank_note_data.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699,   0.     ],\n",
       "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ,   0.     ],\n",
       "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645,   0.     ],\n",
       "       ...,\n",
       "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ,   1.     ],\n",
       "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ,   1.     ],\n",
       "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ,   1.     ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n",
       "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n",
       "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n",
       "       ...,\n",
       "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n",
       "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n",
       "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Test\n",
    "\n",
    "Its time to split the data into a train/test set. Keep in mind, sometimes people like to split 3 ways, train/test/validation. We'll keep things simple for now. **Remember to check out the video explanation as to why we split and what all the parameters mean!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8734  , -0.033118, -0.20165 ,  0.55774 ],\n",
       "       [ 2.0177  ,  1.7982  , -2.9581  ,  0.2099  ],\n",
       "       [-0.36038 ,  4.1158  ,  3.1143  , -0.37199 ],\n",
       "       ...,\n",
       "       [-7.0364  ,  9.2931  ,  0.16594 , -4.5396  ],\n",
       "       [-3.4605  ,  2.6901  ,  0.16165 , -1.0224  ],\n",
       "       [-3.3582  , -7.2404  , 11.4419  , -0.57113 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.5691  ,  6.3465  , -0.1828  , -2.4099  ],\n",
       "       [-0.27802 ,  8.1881  , -3.1338  , -2.5276  ],\n",
       "       [ 0.051979,  7.0521  , -2.0541  , -3.1508  ],\n",
       "       ...,\n",
       "       [ 3.5127  ,  2.9073  ,  1.0579  ,  0.40774 ],\n",
       "       [ 5.504   , 10.3671  , -4.413   , -4.0211  ],\n",
       "       [-0.2062  ,  9.2207  , -3.7044  , -6.8103  ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the Data\n",
    "\n",
    "Usually when using Neural Networks, you will get better performance when you standardize the data. Standardization just means normalizing the values to all fit between a certain range, like 0-1, or -1 to 1.\n",
    "\n",
    "The scikit learn library also provides a nice function for this.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_object = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_object.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler_object.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler_object.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have the data scaled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.9274"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.8734  , -0.033118, -0.20165 ,  0.55774 ],\n",
       "       [ 2.0177  ,  1.7982  , -2.9581  ,  0.2099  ],\n",
       "       [-0.36038 ,  4.1158  ,  3.1143  , -0.37199 ],\n",
       "       ...,\n",
       "       [-7.0364  ,  9.2931  ,  0.16594 , -4.5396  ],\n",
       "       [-3.4605  ,  2.6901  ,  0.16165 , -1.0224  ],\n",
       "       [-3.3582  , -7.2404  , 11.4419  , -0.57113 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.44850688e-01, 5.14130449e-01, 2.18194638e-01, 8.50172258e-01],\n",
       "       [6.53339968e-01, 5.82655745e-01, 9.93242398e-02, 8.17696322e-01],\n",
       "       [4.81846700e-01, 6.69377018e-01, 3.61193167e-01, 7.63368407e-01],\n",
       "       ...,\n",
       "       [4.11050776e-04, 8.63104170e-01, 2.34046756e-01, 3.74261253e-01],\n",
       "       [2.58284115e-01, 6.16029366e-01, 2.33861752e-01, 7.02643151e-01],\n",
       "       [2.65661395e-01, 2.44444278e-01, 7.20316361e-01, 7.44775785e-01]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network with Keras\n",
    "\n",
    "Let's build a simple neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model\n",
    "model = Sequential()\n",
    "# 8 Neurons, expects input of 4 features. \n",
    "# Play around with the number of neurons!!\n",
    "model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "# Add another Densely Connected layer (every neuron connected to every neuron in the next layer)\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# Last layer simple sigmoid function to output 0 or 1 (our label)\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit (Train) the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "29/29 - 0s - loss: 0.0397 - accuracy: 0.9815 - 89ms/epoch - 3ms/step\n",
      "Epoch 2/150\n",
      "29/29 - 0s - loss: 0.0396 - accuracy: 0.9848 - 76ms/epoch - 3ms/step\n",
      "Epoch 3/150\n",
      "29/29 - 0s - loss: 0.0393 - accuracy: 0.9815 - 68ms/epoch - 2ms/step\n",
      "Epoch 4/150\n",
      "29/29 - 0s - loss: 0.0387 - accuracy: 0.9815 - 79ms/epoch - 3ms/step\n",
      "Epoch 5/150\n",
      "29/29 - 0s - loss: 0.0394 - accuracy: 0.9815 - 94ms/epoch - 3ms/step\n",
      "Epoch 6/150\n",
      "29/29 - 0s - loss: 0.0396 - accuracy: 0.9826 - 75ms/epoch - 3ms/step\n",
      "Epoch 7/150\n",
      "29/29 - 0s - loss: 0.0384 - accuracy: 0.9826 - 96ms/epoch - 3ms/step\n",
      "Epoch 8/150\n",
      "29/29 - 0s - loss: 0.0378 - accuracy: 0.9826 - 105ms/epoch - 4ms/step\n",
      "Epoch 9/150\n",
      "29/29 - 0s - loss: 0.0381 - accuracy: 0.9826 - 84ms/epoch - 3ms/step\n",
      "Epoch 10/150\n",
      "29/29 - 0s - loss: 0.0378 - accuracy: 0.9804 - 79ms/epoch - 3ms/step\n",
      "Epoch 11/150\n",
      "29/29 - 0s - loss: 0.0378 - accuracy: 0.9826 - 89ms/epoch - 3ms/step\n",
      "Epoch 12/150\n",
      "29/29 - 0s - loss: 0.0376 - accuracy: 0.9804 - 83ms/epoch - 3ms/step\n",
      "Epoch 13/150\n",
      "29/29 - 0s - loss: 0.0370 - accuracy: 0.9815 - 85ms/epoch - 3ms/step\n",
      "Epoch 14/150\n",
      "29/29 - 0s - loss: 0.0366 - accuracy: 0.9837 - 86ms/epoch - 3ms/step\n",
      "Epoch 15/150\n",
      "29/29 - 0s - loss: 0.0364 - accuracy: 0.9815 - 83ms/epoch - 3ms/step\n",
      "Epoch 16/150\n",
      "29/29 - 0s - loss: 0.0367 - accuracy: 0.9793 - 77ms/epoch - 3ms/step\n",
      "Epoch 17/150\n",
      "29/29 - 0s - loss: 0.0362 - accuracy: 0.9826 - 69ms/epoch - 2ms/step\n",
      "Epoch 18/150\n",
      "29/29 - 0s - loss: 0.0357 - accuracy: 0.9837 - 93ms/epoch - 3ms/step\n",
      "Epoch 19/150\n",
      "29/29 - 0s - loss: 0.0361 - accuracy: 0.9848 - 95ms/epoch - 3ms/step\n",
      "Epoch 20/150\n",
      "29/29 - 0s - loss: 0.0359 - accuracy: 0.9804 - 73ms/epoch - 3ms/step\n",
      "Epoch 21/150\n",
      "29/29 - 0s - loss: 0.0371 - accuracy: 0.9848 - 77ms/epoch - 3ms/step\n",
      "Epoch 22/150\n",
      "29/29 - 0s - loss: 0.0372 - accuracy: 0.9826 - 81ms/epoch - 3ms/step\n",
      "Epoch 23/150\n",
      "29/29 - 0s - loss: 0.0349 - accuracy: 0.9826 - 82ms/epoch - 3ms/step\n",
      "Epoch 24/150\n",
      "29/29 - 0s - loss: 0.0353 - accuracy: 0.9815 - 74ms/epoch - 3ms/step\n",
      "Epoch 25/150\n",
      "29/29 - 0s - loss: 0.0355 - accuracy: 0.9804 - 77ms/epoch - 3ms/step\n",
      "Epoch 26/150\n",
      "29/29 - 0s - loss: 0.0369 - accuracy: 0.9815 - 99ms/epoch - 3ms/step\n",
      "Epoch 27/150\n",
      "29/29 - 0s - loss: 0.0349 - accuracy: 0.9859 - 87ms/epoch - 3ms/step\n",
      "Epoch 28/150\n",
      "29/29 - 0s - loss: 0.0345 - accuracy: 0.9869 - 88ms/epoch - 3ms/step\n",
      "Epoch 29/150\n",
      "29/29 - 0s - loss: 0.0342 - accuracy: 0.9837 - 74ms/epoch - 3ms/step\n",
      "Epoch 30/150\n",
      "29/29 - 0s - loss: 0.0342 - accuracy: 0.9848 - 82ms/epoch - 3ms/step\n",
      "Epoch 31/150\n",
      "29/29 - 0s - loss: 0.0354 - accuracy: 0.9826 - 86ms/epoch - 3ms/step\n",
      "Epoch 32/150\n",
      "29/29 - 0s - loss: 0.0343 - accuracy: 0.9837 - 77ms/epoch - 3ms/step\n",
      "Epoch 33/150\n",
      "29/29 - 0s - loss: 0.0338 - accuracy: 0.9837 - 76ms/epoch - 3ms/step\n",
      "Epoch 34/150\n",
      "29/29 - 0s - loss: 0.0339 - accuracy: 0.9859 - 71ms/epoch - 2ms/step\n",
      "Epoch 35/150\n",
      "29/29 - 0s - loss: 0.0340 - accuracy: 0.9837 - 79ms/epoch - 3ms/step\n",
      "Epoch 36/150\n",
      "29/29 - 0s - loss: 0.0330 - accuracy: 0.9826 - 77ms/epoch - 3ms/step\n",
      "Epoch 37/150\n",
      "29/29 - 0s - loss: 0.0338 - accuracy: 0.9837 - 75ms/epoch - 3ms/step\n",
      "Epoch 38/150\n",
      "29/29 - 0s - loss: 0.0329 - accuracy: 0.9848 - 75ms/epoch - 3ms/step\n",
      "Epoch 39/150\n",
      "29/29 - 0s - loss: 0.0332 - accuracy: 0.9859 - 86ms/epoch - 3ms/step\n",
      "Epoch 40/150\n",
      "29/29 - 0s - loss: 0.0333 - accuracy: 0.9837 - 100ms/epoch - 3ms/step\n",
      "Epoch 41/150\n",
      "29/29 - 0s - loss: 0.0324 - accuracy: 0.9859 - 116ms/epoch - 4ms/step\n",
      "Epoch 42/150\n",
      "29/29 - 0s - loss: 0.0325 - accuracy: 0.9859 - 85ms/epoch - 3ms/step\n",
      "Epoch 43/150\n",
      "29/29 - 0s - loss: 0.0328 - accuracy: 0.9837 - 70ms/epoch - 2ms/step\n",
      "Epoch 44/150\n",
      "29/29 - 0s - loss: 0.0327 - accuracy: 0.9837 - 88ms/epoch - 3ms/step\n",
      "Epoch 45/150\n",
      "29/29 - 0s - loss: 0.0323 - accuracy: 0.9848 - 97ms/epoch - 3ms/step\n",
      "Epoch 46/150\n",
      "29/29 - 0s - loss: 0.0321 - accuracy: 0.9848 - 86ms/epoch - 3ms/step\n",
      "Epoch 47/150\n",
      "29/29 - 0s - loss: 0.0321 - accuracy: 0.9859 - 70ms/epoch - 2ms/step\n",
      "Epoch 48/150\n",
      "29/29 - 0s - loss: 0.0324 - accuracy: 0.9859 - 77ms/epoch - 3ms/step\n",
      "Epoch 49/150\n",
      "29/29 - 0s - loss: 0.0318 - accuracy: 0.9848 - 77ms/epoch - 3ms/step\n",
      "Epoch 50/150\n",
      "29/29 - 0s - loss: 0.0331 - accuracy: 0.9848 - 80ms/epoch - 3ms/step\n",
      "Epoch 51/150\n",
      "29/29 - 0s - loss: 0.0325 - accuracy: 0.9837 - 71ms/epoch - 2ms/step\n",
      "Epoch 52/150\n",
      "29/29 - 0s - loss: 0.0322 - accuracy: 0.9869 - 79ms/epoch - 3ms/step\n",
      "Epoch 53/150\n",
      "29/29 - 0s - loss: 0.0312 - accuracy: 0.9880 - 76ms/epoch - 3ms/step\n",
      "Epoch 54/150\n",
      "29/29 - 0s - loss: 0.0312 - accuracy: 0.9869 - 84ms/epoch - 3ms/step\n",
      "Epoch 55/150\n",
      "29/29 - 0s - loss: 0.0314 - accuracy: 0.9837 - 79ms/epoch - 3ms/step\n",
      "Epoch 56/150\n",
      "29/29 - 0s - loss: 0.0315 - accuracy: 0.9859 - 76ms/epoch - 3ms/step\n",
      "Epoch 57/150\n",
      "29/29 - 0s - loss: 0.0313 - accuracy: 0.9826 - 72ms/epoch - 2ms/step\n",
      "Epoch 58/150\n",
      "29/29 - 0s - loss: 0.0323 - accuracy: 0.9837 - 73ms/epoch - 3ms/step\n",
      "Epoch 59/150\n",
      "29/29 - 0s - loss: 0.0304 - accuracy: 0.9880 - 72ms/epoch - 2ms/step\n",
      "Epoch 60/150\n",
      "29/29 - 0s - loss: 0.0304 - accuracy: 0.9869 - 72ms/epoch - 2ms/step\n",
      "Epoch 61/150\n",
      "29/29 - 0s - loss: 0.0313 - accuracy: 0.9859 - 68ms/epoch - 2ms/step\n",
      "Epoch 62/150\n",
      "29/29 - 0s - loss: 0.0310 - accuracy: 0.9859 - 67ms/epoch - 2ms/step\n",
      "Epoch 63/150\n",
      "29/29 - 0s - loss: 0.0303 - accuracy: 0.9869 - 81ms/epoch - 3ms/step\n",
      "Epoch 64/150\n",
      "29/29 - 0s - loss: 0.0304 - accuracy: 0.9869 - 84ms/epoch - 3ms/step\n",
      "Epoch 65/150\n",
      "29/29 - 0s - loss: 0.0299 - accuracy: 0.9869 - 77ms/epoch - 3ms/step\n",
      "Epoch 66/150\n",
      "29/29 - 0s - loss: 0.0298 - accuracy: 0.9891 - 73ms/epoch - 3ms/step\n",
      "Epoch 67/150\n",
      "29/29 - 0s - loss: 0.0298 - accuracy: 0.9869 - 70ms/epoch - 2ms/step\n",
      "Epoch 68/150\n",
      "29/29 - 0s - loss: 0.0297 - accuracy: 0.9891 - 84ms/epoch - 3ms/step\n",
      "Epoch 69/150\n",
      "29/29 - 0s - loss: 0.0302 - accuracy: 0.9880 - 83ms/epoch - 3ms/step\n",
      "Epoch 70/150\n",
      "29/29 - 0s - loss: 0.0307 - accuracy: 0.9848 - 67ms/epoch - 2ms/step\n",
      "Epoch 71/150\n",
      "29/29 - 0s - loss: 0.0297 - accuracy: 0.9859 - 70ms/epoch - 2ms/step\n",
      "Epoch 72/150\n",
      "29/29 - 0s - loss: 0.0299 - accuracy: 0.9869 - 137ms/epoch - 5ms/step\n",
      "Epoch 73/150\n",
      "29/29 - 0s - loss: 0.0301 - accuracy: 0.9869 - 84ms/epoch - 3ms/step\n",
      "Epoch 74/150\n",
      "29/29 - 0s - loss: 0.0307 - accuracy: 0.9859 - 65ms/epoch - 2ms/step\n",
      "Epoch 75/150\n",
      "29/29 - 0s - loss: 0.0308 - accuracy: 0.9848 - 75ms/epoch - 3ms/step\n",
      "Epoch 76/150\n",
      "29/29 - 0s - loss: 0.0302 - accuracy: 0.9891 - 82ms/epoch - 3ms/step\n",
      "Epoch 77/150\n",
      "29/29 - 0s - loss: 0.0289 - accuracy: 0.9880 - 71ms/epoch - 2ms/step\n",
      "Epoch 78/150\n",
      "29/29 - 0s - loss: 0.0290 - accuracy: 0.9869 - 68ms/epoch - 2ms/step\n",
      "Epoch 79/150\n",
      "29/29 - 0s - loss: 0.0289 - accuracy: 0.9869 - 68ms/epoch - 2ms/step\n",
      "Epoch 80/150\n",
      "29/29 - 0s - loss: 0.0291 - accuracy: 0.9869 - 66ms/epoch - 2ms/step\n",
      "Epoch 81/150\n",
      "29/29 - 0s - loss: 0.0291 - accuracy: 0.9859 - 76ms/epoch - 3ms/step\n",
      "Epoch 82/150\n",
      "29/29 - 0s - loss: 0.0292 - accuracy: 0.9880 - 88ms/epoch - 3ms/step\n",
      "Epoch 83/150\n",
      "29/29 - 0s - loss: 0.0289 - accuracy: 0.9859 - 82ms/epoch - 3ms/step\n",
      "Epoch 84/150\n",
      "29/29 - 0s - loss: 0.0288 - accuracy: 0.9880 - 83ms/epoch - 3ms/step\n",
      "Epoch 85/150\n",
      "29/29 - 0s - loss: 0.0283 - accuracy: 0.9891 - 87ms/epoch - 3ms/step\n",
      "Epoch 86/150\n",
      "29/29 - 0s - loss: 0.0284 - accuracy: 0.9880 - 74ms/epoch - 3ms/step\n",
      "Epoch 87/150\n",
      "29/29 - 0s - loss: 0.0293 - accuracy: 0.9891 - 66ms/epoch - 2ms/step\n",
      "Epoch 88/150\n",
      "29/29 - 0s - loss: 0.0286 - accuracy: 0.9880 - 72ms/epoch - 2ms/step\n",
      "Epoch 89/150\n",
      "29/29 - 0s - loss: 0.0293 - accuracy: 0.9837 - 75ms/epoch - 3ms/step\n",
      "Epoch 90/150\n",
      "29/29 - 0s - loss: 0.0288 - accuracy: 0.9913 - 82ms/epoch - 3ms/step\n",
      "Epoch 91/150\n",
      "29/29 - 0s - loss: 0.0289 - accuracy: 0.9880 - 82ms/epoch - 3ms/step\n",
      "Epoch 92/150\n",
      "29/29 - 0s - loss: 0.0282 - accuracy: 0.9902 - 80ms/epoch - 3ms/step\n",
      "Epoch 93/150\n",
      "29/29 - 0s - loss: 0.0283 - accuracy: 0.9913 - 74ms/epoch - 3ms/step\n",
      "Epoch 94/150\n",
      "29/29 - 0s - loss: 0.0281 - accuracy: 0.9902 - 111ms/epoch - 4ms/step\n",
      "Epoch 95/150\n",
      "29/29 - 0s - loss: 0.0280 - accuracy: 0.9891 - 101ms/epoch - 3ms/step\n",
      "Epoch 96/150\n",
      "29/29 - 0s - loss: 0.0279 - accuracy: 0.9902 - 86ms/epoch - 3ms/step\n",
      "Epoch 97/150\n",
      "29/29 - 0s - loss: 0.0286 - accuracy: 0.9848 - 79ms/epoch - 3ms/step\n",
      "Epoch 98/150\n",
      "29/29 - 0s - loss: 0.0281 - accuracy: 0.9891 - 91ms/epoch - 3ms/step\n",
      "Epoch 99/150\n",
      "29/29 - 0s - loss: 0.0299 - accuracy: 0.9848 - 93ms/epoch - 3ms/step\n",
      "Epoch 100/150\n",
      "29/29 - 0s - loss: 0.0276 - accuracy: 0.9869 - 104ms/epoch - 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/150\n",
      "29/29 - 0s - loss: 0.0274 - accuracy: 0.9869 - 82ms/epoch - 3ms/step\n",
      "Epoch 102/150\n",
      "29/29 - 0s - loss: 0.0280 - accuracy: 0.9859 - 86ms/epoch - 3ms/step\n",
      "Epoch 103/150\n",
      "29/29 - 0s - loss: 0.0289 - accuracy: 0.9891 - 86ms/epoch - 3ms/step\n",
      "Epoch 104/150\n",
      "29/29 - 0s - loss: 0.0277 - accuracy: 0.9869 - 90ms/epoch - 3ms/step\n",
      "Epoch 105/150\n",
      "29/29 - 0s - loss: 0.0274 - accuracy: 0.9902 - 109ms/epoch - 4ms/step\n",
      "Epoch 106/150\n",
      "29/29 - 0s - loss: 0.0273 - accuracy: 0.9902 - 84ms/epoch - 3ms/step\n",
      "Epoch 107/150\n",
      "29/29 - 0s - loss: 0.0270 - accuracy: 0.9913 - 79ms/epoch - 3ms/step\n",
      "Epoch 108/150\n",
      "29/29 - 0s - loss: 0.0274 - accuracy: 0.9891 - 96ms/epoch - 3ms/step\n",
      "Epoch 109/150\n",
      "29/29 - 0s - loss: 0.0271 - accuracy: 0.9891 - 92ms/epoch - 3ms/step\n",
      "Epoch 110/150\n",
      "29/29 - 0s - loss: 0.0273 - accuracy: 0.9902 - 89ms/epoch - 3ms/step\n",
      "Epoch 111/150\n",
      "29/29 - 0s - loss: 0.0276 - accuracy: 0.9913 - 84ms/epoch - 3ms/step\n",
      "Epoch 112/150\n",
      "29/29 - 0s - loss: 0.0278 - accuracy: 0.9902 - 81ms/epoch - 3ms/step\n",
      "Epoch 113/150\n",
      "29/29 - 0s - loss: 0.0273 - accuracy: 0.9891 - 83ms/epoch - 3ms/step\n",
      "Epoch 114/150\n",
      "29/29 - 0s - loss: 0.0277 - accuracy: 0.9880 - 93ms/epoch - 3ms/step\n",
      "Epoch 115/150\n",
      "29/29 - 0s - loss: 0.0275 - accuracy: 0.9891 - 99ms/epoch - 3ms/step\n",
      "Epoch 116/150\n",
      "29/29 - 0s - loss: 0.0268 - accuracy: 0.9891 - 88ms/epoch - 3ms/step\n",
      "Epoch 117/150\n",
      "29/29 - 0s - loss: 0.0270 - accuracy: 0.9902 - 87ms/epoch - 3ms/step\n",
      "Epoch 118/150\n",
      "29/29 - 0s - loss: 0.0272 - accuracy: 0.9869 - 82ms/epoch - 3ms/step\n",
      "Epoch 119/150\n",
      "29/29 - 0s - loss: 0.0269 - accuracy: 0.9902 - 81ms/epoch - 3ms/step\n",
      "Epoch 120/150\n",
      "29/29 - 0s - loss: 0.0262 - accuracy: 0.9891 - 85ms/epoch - 3ms/step\n",
      "Epoch 121/150\n",
      "29/29 - 0s - loss: 0.0264 - accuracy: 0.9891 - 85ms/epoch - 3ms/step\n",
      "Epoch 122/150\n",
      "29/29 - 0s - loss: 0.0263 - accuracy: 0.9859 - 117ms/epoch - 4ms/step\n",
      "Epoch 123/150\n",
      "29/29 - 0s - loss: 0.0270 - accuracy: 0.9913 - 94ms/epoch - 3ms/step\n",
      "Epoch 124/150\n",
      "29/29 - 0s - loss: 0.0270 - accuracy: 0.9891 - 83ms/epoch - 3ms/step\n",
      "Epoch 125/150\n",
      "29/29 - 0s - loss: 0.0264 - accuracy: 0.9891 - 80ms/epoch - 3ms/step\n",
      "Epoch 126/150\n",
      "29/29 - 0s - loss: 0.0265 - accuracy: 0.9891 - 93ms/epoch - 3ms/step\n",
      "Epoch 127/150\n",
      "29/29 - 0s - loss: 0.0257 - accuracy: 0.9913 - 85ms/epoch - 3ms/step\n",
      "Epoch 128/150\n",
      "29/29 - 0s - loss: 0.0265 - accuracy: 0.9880 - 84ms/epoch - 3ms/step\n",
      "Epoch 129/150\n",
      "29/29 - 0s - loss: 0.0261 - accuracy: 0.9902 - 95ms/epoch - 3ms/step\n",
      "Epoch 130/150\n",
      "29/29 - 0s - loss: 0.0260 - accuracy: 0.9913 - 113ms/epoch - 4ms/step\n",
      "Epoch 131/150\n",
      "29/29 - 0s - loss: 0.0259 - accuracy: 0.9913 - 79ms/epoch - 3ms/step\n",
      "Epoch 132/150\n",
      "29/29 - 0s - loss: 0.0262 - accuracy: 0.9902 - 85ms/epoch - 3ms/step\n",
      "Epoch 133/150\n",
      "29/29 - 0s - loss: 0.0263 - accuracy: 0.9891 - 83ms/epoch - 3ms/step\n",
      "Epoch 134/150\n",
      "29/29 - 0s - loss: 0.0262 - accuracy: 0.9891 - 93ms/epoch - 3ms/step\n",
      "Epoch 135/150\n",
      "29/29 - 0s - loss: 0.0255 - accuracy: 0.9891 - 83ms/epoch - 3ms/step\n",
      "Epoch 136/150\n",
      "29/29 - 0s - loss: 0.0270 - accuracy: 0.9848 - 88ms/epoch - 3ms/step\n",
      "Epoch 137/150\n",
      "29/29 - 0s - loss: 0.0268 - accuracy: 0.9891 - 99ms/epoch - 3ms/step\n",
      "Epoch 138/150\n",
      "29/29 - 0s - loss: 0.0254 - accuracy: 0.9891 - 91ms/epoch - 3ms/step\n",
      "Epoch 139/150\n",
      "29/29 - 0s - loss: 0.0259 - accuracy: 0.9891 - 84ms/epoch - 3ms/step\n",
      "Epoch 140/150\n",
      "29/29 - 0s - loss: 0.0255 - accuracy: 0.9902 - 82ms/epoch - 3ms/step\n",
      "Epoch 141/150\n",
      "29/29 - 0s - loss: 0.0282 - accuracy: 0.9859 - 83ms/epoch - 3ms/step\n",
      "Epoch 142/150\n",
      "29/29 - 0s - loss: 0.0254 - accuracy: 0.9859 - 88ms/epoch - 3ms/step\n",
      "Epoch 143/150\n",
      "29/29 - 0s - loss: 0.0271 - accuracy: 0.9880 - 81ms/epoch - 3ms/step\n",
      "Epoch 144/150\n",
      "29/29 - 0s - loss: 0.0251 - accuracy: 0.9880 - 82ms/epoch - 3ms/step\n",
      "Epoch 145/150\n",
      "29/29 - 0s - loss: 0.0264 - accuracy: 0.9880 - 83ms/epoch - 3ms/step\n",
      "Epoch 146/150\n",
      "29/29 - 0s - loss: 0.0254 - accuracy: 0.9880 - 83ms/epoch - 3ms/step\n",
      "Epoch 147/150\n",
      "29/29 - 0s - loss: 0.0252 - accuracy: 0.9902 - 91ms/epoch - 3ms/step\n",
      "Epoch 148/150\n",
      "29/29 - 0s - loss: 0.0261 - accuracy: 0.9891 - 92ms/epoch - 3ms/step\n",
      "Epoch 149/150\n",
      "29/29 - 0s - loss: 0.0260 - accuracy: 0.9880 - 83ms/epoch - 3ms/step\n",
      "Epoch 150/150\n",
      "29/29 - 0s - loss: 0.0252 - accuracy: 0.9913 - 82ms/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e9af3906d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play around with number of epochs as well!\n",
    "model.fit(scaled_X_train,y_train,epochs=150, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting New Unseen Data\n",
    "\n",
    "Let's see how we did by predicting on **new data**. Remember, our model has **never** seen the test data that we scaled previously! This process is the exact same process you would use on totally brand new data. For example , a brand new bank note that you just analyzed ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62098955, 0.75284662, 0.21900753, 0.5730998 ],\n",
       "       [0.48778602, 0.82175665, 0.09174727, 0.56211079],\n",
       "       [0.51158363, 0.77924916, 0.13830875, 0.50392598],\n",
       "       ...,\n",
       "       [0.76115065, 0.62415668, 0.27251204, 0.83616757],\n",
       "       [0.9047516 , 0.90329171, 0.03658247, 0.42267079],\n",
       "       [0.49296526, 0.86039507, 0.06714046, 0.1622583 ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spits out probabilities by default.\n",
    "# model.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predict_classes(scaled_X_test)\n",
    "(model.predict(X_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance\n",
    "\n",
    "So how well did we do? How do we actually measure \"well\". Is 95% accuracy good enough? It all depends on the situation. Also we need to take into account things like recall and precision. Make sure to watch the video discussion on classification evaluation before running this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.027535848319530487, 0.9889624714851379]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=scaled_X_test,y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# predictions = model.predict_classes(scaled_X_test)\n",
    "predictions = (model.predict(scaled_X_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254,   3],\n",
       "       [  2, 194]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       257\n",
      "         1.0       0.98      0.99      0.99       196\n",
      "\n",
      "    accuracy                           0.99       453\n",
      "   macro avg       0.99      0.99      0.99       453\n",
      "weighted avg       0.99      0.99      0.99       453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "Now that we have a model trained, let's see how we can save and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = load_model('myfirstmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# newmodel.predict_classes(X_test)\n",
    "newmodel.predict(X_test > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You now know how to preprocess data, train a neural network, and evaluate its classification performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
